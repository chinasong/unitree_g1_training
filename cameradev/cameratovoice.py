import time
import sys
import json
import cv2
from ultralytics import YOLO

from unitree_sdk2py.core.channel import ChannelFactoryInitialize
from unitree_sdk2py.g1.audio.g1_audio_client import AudioClient

def init_audio_client(net_iface: str):
    ChannelFactoryInitialize(0, net_iface)
    audio_client = AudioClient()
    audio_client.SetTimeout(10.0)
    audio_client.Init()
    audio_client.SetVolume(85)
    return audio_client

def speak(audio_client, text):
    print(f"ğŸ“¢ æ’­æŠ¥å†…å®¹ï¼š{text}")
    audio_client.TtsMaker(text, 0)

def load_label_map(file_path: str):
    with open(file_path, "r", encoding="utf-8") as f:
        return json.load(f)

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print(f"Usage: python3 {sys.argv[0]} <networkInterface>")
        sys.exit(-1)

    net_iface = sys.argv[1]
    audio_client = init_audio_client(net_iface)

    # åŠ è½½ YOLO æ¨¡å‹
    model = YOLO("yolo11n.pt")
    names = model.names

    # åŠ è½½è‹±æ–‡ -> ä¸­æ–‡æ ‡ç­¾æ˜ å°„
    en_to_zh = load_label_map("en_to_zh.js")

    # æ‰“å¼€æ‘„åƒå¤´
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ æ‘„åƒå¤´æ‰“å¼€å¤±è´¥")
        sys.exit(-1)

    print("âœ… å¼€å§‹è¯†åˆ«ç‰©ä½“å¹¶æ’­æŠ¥ï¼ŒæŒ‰ q é€€å‡º")

    last_labels = set()
    last_spoken_time = time.time()

    while True:
        ret, frame = cap.read()
        if not ret:
            continue

        results = model(frame)
        boxes = results[0].boxes

        if boxes is None or len(boxes) == 0:
            continue

        # æå–é¢„æµ‹ç±»åˆ«ï¼ŒæŒ‰ç½®ä¿¡åº¦æ’åº
        sorted_labels = [
            (names[int(cls)], float(score))
            for cls, score in zip(boxes.cls, boxes.conf)
        ]
        sorted_labels.sort(key=lambda x: x[1], reverse=True)

        # å–å‰5ä¸ªæœ€å¯ä¿¡çš„æ ‡ç­¾
        current_labels = set([label for label, _ in sorted_labels[:5]])
        current_zh_labels = set([en_to_zh.get(label, label) for label in current_labels])

        if current_zh_labels != last_labels and time.time() - last_spoken_time > 3:
            text = "æˆ‘çœ‹åˆ°äº†ï¼š" + "ã€".join(current_zh_labels)
            speak(audio_client, text)
            last_labels = current_zh_labels
            last_spoken_time = time.time()

        annotated = results[0].plot()
        cv2.imshow("G1 Camera", annotated)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()